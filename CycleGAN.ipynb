{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOf4UtxWaBosyV+LClA5U16",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NewAnik/DataScience/blob/master/CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtfUU-B-KugO"
      },
      "source": [
        "# loading in and transforming data\r\n",
        "import os\r\n",
        "import torch\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torchvision\r\n",
        "import torchvision.datasets as datasets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "\r\n",
        "# visualizing data\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import warnings\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI5hVApsLAOF"
      },
      "source": [
        "def get_data_loader(image_type, image_dir='summer2winter_yosemite', \r\n",
        "                    image_size=128, batch_size=16, num_workers=0):\r\n",
        "    \"\"\"Returns training and test data loaders for a given image type, either 'summer' or 'winter'. \r\n",
        "       These images will be resized to 128x128x3, by default, converted into Tensors, and normalized.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # resize and normalize the images\r\n",
        "    transform = transforms.Compose([transforms.Resize(image_size), # resize to 128x128\r\n",
        "                                    transforms.ToTensor()])\r\n",
        "\r\n",
        "    # get training and test directories\r\n",
        "    image_path = './' + image_dir\r\n",
        "    train_path = os.path.join(image_path, image_type)\r\n",
        "    test_path = os.path.join(image_path, 'test_{}'.format(image_type))\r\n",
        "\r\n",
        "    # define datasets using ImageFolder\r\n",
        "    train_dataset = datasets.ImageFolder(train_path, transform)\r\n",
        "    test_dataset = datasets.ImageFolder(test_path, transform)\r\n",
        "\r\n",
        "    # create and return DataLoaders\r\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\r\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\r\n",
        "\r\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "JfbKIcqVLFvS",
        "outputId": "12379575-987d-4be9-b474-846c8b7fa203"
      },
      "source": [
        "# Create train and test dataloaders for images from the two domains X and Y\r\n",
        "# image_type = directory names for our data\r\n",
        "dataloader_X, test_dataloader_X = get_data_loader(image_type='summer')\r\n",
        "dataloader_Y, test_dataloader_Y = get_data_loader(image_type='winter')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d8b89d884217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create train and test dataloaders for images from the two domains X and Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# image_type = directory names for our data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataloader_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'summer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataloader_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'winter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-3dd25f88717d>\u001b[0m in \u001b[0;36mget_data_loader\u001b[0;34m(image_type, image_dir, image_size, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# define datasets using ImageFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    106\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    107\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './summer2winter_yosemite/summer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XtpX7bFw2Cl"
      },
      "source": [
        "# helper imshow function\r\n",
        "def imshow(img):\r\n",
        "    npimg = img.numpy()\r\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\r\n",
        "    \r\n",
        "\r\n",
        "# get some images from X\r\n",
        "dataiter = iter(dataloader_X)\r\n",
        "# the \"_\" is a placeholder for no labels\r\n",
        "images, _ = dataiter.next()\r\n",
        "\r\n",
        "# show images\r\n",
        "fig = plt.figure(figsize=(15, 8))\r\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0kfxEEbw6pQ"
      },
      "source": [
        "# get some images from Y\r\n",
        "dataiter = iter(dataloader_Y)\r\n",
        "images, _ = dataiter.next()\r\n",
        "\r\n",
        "# show images\r\n",
        "fig = plt.figure(figsize=(15,8))\r\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaBdFSdQw-4L"
      },
      "source": [
        "# current range\r\n",
        "img = images[0]\r\n",
        "\r\n",
        "print('Min: ', img.min())\r\n",
        "print('Max: ', img.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pi-AV7Cijms"
      },
      "source": [
        "# helper scale function\r\n",
        "def scale(x, feature_range=(-1, 1)):\r\n",
        "    ''' Scale takes in an image x and returns that image, scaled\r\n",
        "       with a feature_range of pixel values from -1 to 1. \r\n",
        "       This function assumes that the input x is already scaled from 0-1.'''\r\n",
        "    \r\n",
        "    # scale from 0-1 to feature_range\r\n",
        "    min, max = feature_range\r\n",
        "    x = x * (max - min) + min\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhSHHkWbil2Z"
      },
      "source": [
        "# scaled range\r\n",
        "scaled_img = scale(img)\r\n",
        "\r\n",
        "print('Scaled min: ', scaled_img.min())\r\n",
        "print('Scaled max: ', scaled_img.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH96Brc2ip_A"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "# helper conv function\r\n",
        "def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\r\n",
        "    \"\"\"Creates a convolutional layer, with optional batch normalization.\r\n",
        "    \"\"\"\r\n",
        "    layers = []\r\n",
        "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \r\n",
        "                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\r\n",
        "    \r\n",
        "    layers.append(conv_layer)\r\n",
        "\r\n",
        "    if batch_norm:\r\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\r\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf-oEGaQiuzR"
      },
      "source": [
        "class Discriminator(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, conv_dim=64):\r\n",
        "        super(Discriminator, self).__init__()\r\n",
        "\r\n",
        "        # Define all convolutional layers\r\n",
        "        # Should accept an RGB image as input and output a single value\r\n",
        "        self.layer_1 = conv(3,conv_dim,4,batch_norm = False)\r\n",
        "        self.layer_2 = conv(conv_dim,conv_dim*2,4)\r\n",
        "        self.layer_3 = conv(conv_dim*2,conv_dim*4,4)\r\n",
        "        self.layer_4 = conv(conv_dim*4,conv_dim*8,4)\r\n",
        "        self.layer_5 = conv(conv_dim*8,1,4,1,batch_norm = False)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # define feedforward behavior\r\n",
        "        x = F.relu(self.layer_1(x))\r\n",
        "        x = F.relu(self.layer_2(x))\r\n",
        "        x = F.relu(self.layer_3(x))\r\n",
        "        x = F.relu(self.layer_4(x))\r\n",
        "        \r\n",
        "        x = self.layer_5(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQQ30i9Ei1sJ"
      },
      "source": [
        "# residual block class\r\n",
        "class ResidualBlock(nn.Module):\r\n",
        "    \"\"\"Defines a residual block.\r\n",
        "       This adds an input x to a convolutional layer (applied to x) with the same size input and output.\r\n",
        "       These blocks allow a model to learn an effective transformation from one domain to another.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, conv_dim):\r\n",
        "        super(ResidualBlock, self).__init__()\r\n",
        "        # conv_dim = number of inputs  \r\n",
        "        \r\n",
        "        # define two convolutional layers + batch normalization that will act as our residual function, F(x)\r\n",
        "        # layers should have the same shape input as output; I suggest a kernel_size of 3\r\n",
        "        self.layer_1 = conv(conv_dim,conv_dim,3,1,1,batch_norm = True)\r\n",
        "        self.layer_2 = conv(conv_dim,conv_dim,3,1,1,batch_norm = True)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        # apply a ReLu activation the outputs of the first layer\r\n",
        "        # return a summed output, x + resnet_block(x)\r\n",
        "        out_1 = F.relu(self.layer_1(x))\r\n",
        "        out_2 = x + self.layer_2(out_1)\r\n",
        "        \r\n",
        "        return out_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8-_i0X8jauG"
      },
      "source": [
        "# helper deconv function\r\n",
        "def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\r\n",
        "    \"\"\"Creates a transpose convolutional layer, with optional batch normalization.\r\n",
        "    \"\"\"\r\n",
        "    layers = []\r\n",
        "    # append transpose conv layer\r\n",
        "    layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False))\r\n",
        "    # optional batch norm layer\r\n",
        "    if batch_norm:\r\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\r\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQV7lNv2jgtH"
      },
      "source": [
        "    \r\n",
        "    def __init__(self, conv_dim=64, n_res_blocks=6):\r\n",
        "        super(CycleGenerator, self).__init__()\r\n",
        "\r\n",
        "        # 1. Define the encoder part of the generator\r\n",
        "        self.layer_1 = conv(3,conv_dim,4)\r\n",
        "        self.layer_2 = conv(conv_dim,conv_dim*2,4)\r\n",
        "        self.layer_3 = conv(conv_dim*2,conv_dim*4,4)\r\n",
        "        # 2. Define the resnet part of the generator\r\n",
        "        layers = []\r\n",
        "        for n in range(n_res_blocks):\r\n",
        "            layers.append(ResidualBlock(conv_dim*4))\r\n",
        "        self.res_blocks = nn.Sequential(*layers)\r\n",
        "        # 3. Define the decoder part of the generator\r\n",
        "        self.layer_4 = deconv(conv_dim*4,conv_dim*2,4)\r\n",
        "        self.layer_5 = deconv(conv_dim*2,conv_dim,4)\r\n",
        "        self.layer_6 = deconv(conv_dim,3,4,batch_norm = False)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \"\"\"Given an image x, returns a transformed image.\"\"\"\r\n",
        "        # define feedforward behavior, applying activations as necessary\r\n",
        "        \r\n",
        "        out = F.relu(self.layer_1(x))\r\n",
        "        out = F.relu(self.layer_2(out))\r\n",
        "        out = F.relu(self.layer_3(out))\r\n",
        "        \r\n",
        "        out = self.res_blocks(out)\r\n",
        "        \r\n",
        "        out = F.relu(self.layer_4(out))\r\n",
        "        out = F.relu(self.layer_5(out))\r\n",
        "        out = F.tanh(self.layer_6(out))\r\n",
        "        \r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-vxgH-Yjoq9"
      },
      "source": [
        "def create_model(g_conv_dim=64, d_conv_dim=64, n_res_blocks=6):\r\n",
        "    \"\"\"Builds the generators and discriminators.\"\"\"\r\n",
        "    \r\n",
        "    # Instantiate generators\r\n",
        "    G_XtoY = CycleGenerator(g_conv_dim,n_res_blocks)\r\n",
        "    G_YtoX = CycleGenerator(g_conv_dim,n_res_blocks)\r\n",
        "    # Instantiate discriminators\r\n",
        "    D_X = Discriminator(d_conv_dim)\r\n",
        "    D_Y = Discriminator(d_conv_dim)\r\n",
        "    \r\n",
        "\r\n",
        "    # move models to GPU, if available\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        device = torch.device(\"cuda:0\")\r\n",
        "        G_XtoY.to(device)\r\n",
        "        G_YtoX.to(device)\r\n",
        "        D_X.to(device)\r\n",
        "        D_Y.to(device)\r\n",
        "        print('Models moved to GPU.')\r\n",
        "    else:\r\n",
        "        print('Only CPU available.')\r\n",
        "\r\n",
        "    return G_XtoY, G_YtoX, D_X, D_Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYCvTXkUjtm3"
      },
      "source": [
        "# call the function to get models\r\n",
        "G_XtoY, G_YtoX, D_X, D_Y = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFrPof_ujwmm"
      },
      "source": [
        "# helper function for printing the model architecture\r\n",
        "def print_models(G_XtoY, G_YtoX, D_X, D_Y):\r\n",
        "    \"\"\"Prints model information for the generators and discriminators.\r\n",
        "    \"\"\"\r\n",
        "    print(\"                     G_XtoY                    \")\r\n",
        "    print(\"-----------------------------------------------\")\r\n",
        "    print(G_XtoY)\r\n",
        "    print()\r\n",
        "\r\n",
        "    print(\"                     G_YtoX                    \")\r\n",
        "    print(\"-----------------------------------------------\")\r\n",
        "    print(G_YtoX)\r\n",
        "    print()\r\n",
        "\r\n",
        "    print(\"                      D_X                      \")\r\n",
        "    print(\"-----------------------------------------------\")\r\n",
        "    print(D_X)\r\n",
        "    print()\r\n",
        "\r\n",
        "    print(\"                      D_Y                      \")\r\n",
        "    print(\"-----------------------------------------------\")\r\n",
        "    print(D_Y)\r\n",
        "    print()\r\n",
        "    \r\n",
        "\r\n",
        "# print all of the models\r\n",
        "print_models(G_XtoY, G_YtoX, D_X, D_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sXUXSsNj3pS"
      },
      "source": [
        "def real_mse_loss(D_out):\r\n",
        "    # how close is the produced output from being \"real\"?\r\n",
        "    return torch.mean((D_out - 1)**2)\r\n",
        "\r\n",
        "    \r\n",
        "def fake_mse_loss(D_out):\r\n",
        "    # how close is the produced output from being \"fake\"?\r\n",
        "    return torch.mean(D_out**2)\r\n",
        "\r\n",
        "def cycle_consistency_loss(real_im, reconstructed_im, lambda_weight):\r\n",
        "    # calculate reconstruction loss \r\n",
        "    # return weighted loss\r\n",
        "    loss = torch.mean(torch.abs(real_im - reconstructed_im))\r\n",
        "    return loss*lambda_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WHEW0QEkAHO"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "# hyperparams for Adam optimizers\r\n",
        "lr= 0.0002\r\n",
        "beta1= 0.5\r\n",
        "beta2= 0.999\r\n",
        "\r\n",
        "g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters\r\n",
        "\r\n",
        "# Create optimizers for the generators and discriminators\r\n",
        "g_optimizer = optim.Adam(g_params, lr, [beta1, beta2])\r\n",
        "d_x_optimizer = optim.Adam(D_X.parameters(), lr, [beta1, beta2])\r\n",
        "d_y_optimizer = optim.Adam(D_Y.parameters(), lr, [beta1, beta2])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCbnr9QtkafS"
      },
      "source": [
        "# import save code\r\n",
        "from helpers import save_samples, checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0iMg-Lzkeby"
      },
      "source": [
        "# train the network\r\n",
        "def training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, \r\n",
        "                  n_epochs=1000):\r\n",
        "    \r\n",
        "    print_every=10\r\n",
        "    \r\n",
        "    # keep track of losses over time\r\n",
        "    losses = []\r\n",
        "\r\n",
        "    test_iter_X = iter(test_dataloader_X)\r\n",
        "    test_iter_Y = iter(test_dataloader_Y)\r\n",
        "\r\n",
        "    # Get some fixed data from domains X and Y for sampling. These are images that are held\r\n",
        "    # constant throughout training, that allow us to inspect the model's performance.\r\n",
        "    fixed_X = test_iter_X.next()[0]\r\n",
        "    fixed_Y = test_iter_Y.next()[0]\r\n",
        "    fixed_X = scale(fixed_X) # make sure to scale to a range -1 to 1\r\n",
        "    fixed_Y = scale(fixed_Y)\r\n",
        "\r\n",
        "    # batches per epoch\r\n",
        "    iter_X = iter(dataloader_X)\r\n",
        "    iter_Y = iter(dataloader_Y)\r\n",
        "    batches_per_epoch = min(len(iter_X), len(iter_Y))\r\n",
        "\r\n",
        "    for epoch in range(1, n_epochs+1):\r\n",
        "\r\n",
        "        # Reset iterators for each epoch\r\n",
        "        if epoch % batches_per_epoch == 0:\r\n",
        "            iter_X = iter(dataloader_X)\r\n",
        "            iter_Y = iter(dataloader_Y)\r\n",
        "\r\n",
        "        images_X, _ = iter_X.next()\r\n",
        "        images_X = scale(images_X) # make sure to scale to a range -1 to 1\r\n",
        "\r\n",
        "        images_Y, _ = iter_Y.next()\r\n",
        "        images_Y = scale(images_Y)\r\n",
        "        # move images to GPU if available (otherwise stay on CPU)\r\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "        images_X = images_X.to(device)\r\n",
        "        images_Y = images_Y.to(device)\r\n",
        "\r\n",
        "\r\n",
        "        # ============================================\r\n",
        "        #            TRAIN THE DISCRIMINATORS\r\n",
        "        # ============================================\r\n",
        "\r\n",
        "        ##   First: D_X, real and fake loss components   ##\r\n",
        "\r\n",
        "        # 1. Compute the discriminator losses on real images\r\n",
        "        d_x_optimizer.zero_grad()\r\n",
        "        real_D_loss = real_mse_loss(D_X(images_X))\r\n",
        "        # 3. Compute the fake loss for D_X\r\n",
        "        fake_D_loss = fake_mse_loss(D_X(G_YtoX(images_Y)))\r\n",
        "        # 4. Compute the total loss and perform backprop\r\n",
        "        d_x_loss = real_D_loss + fake_D_loss\r\n",
        "        d_x_loss.backward()\r\n",
        "        d_x_optimizer.step()\r\n",
        "        \r\n",
        "        ##   Second: D_Y, real and fake loss components   ##\r\n",
        "        d_y_optimizer.zero_grad()\r\n",
        "        real_D_y_loss = real_mse_loss(D_Y(images_Y))\r\n",
        "        \r\n",
        "        fake_D_y_loss = fake_mse_loss(D_Y(G_XtoY(images_X)))\r\n",
        "        \r\n",
        "        d_y_loss = real_D_y_loss + fake_D_y_loss\r\n",
        "        d_y_loss.backward()\r\n",
        "        d_y_optimizer.step()\r\n",
        "\r\n",
        "\r\n",
        "        # =========================================\r\n",
        "        #            TRAIN THE GENERATORS\r\n",
        "        # =========================================\r\n",
        "\r\n",
        "        ##    First: generate fake X images and reconstructed Y images    ##\r\n",
        "        g_optimizer.zero_grad()\r\n",
        "        # 1. Generate fake images that look like domain X based on real images in domain Y\r\n",
        "        out_1 = G_YtoX(images_Y)\r\n",
        "        # 2. Compute the generator loss based on domain X\r\n",
        "        loss_1 = real_mse_loss(D_X(out_1))\r\n",
        "        # 3. Create a reconstructed y\r\n",
        "        out_2 = G_XtoY(out_1)\r\n",
        "        # 4. Compute the cycle consistency loss (the reconstruction loss)\r\n",
        "        loss_2 = cycle_consistency_loss(real_im = images_Y, reconstructed_im = out_2, lambda_weight=10)\r\n",
        "\r\n",
        "        ##    Second: generate fake Y images and reconstructed X images    ##\r\n",
        "        out_3 = G_XtoY(images_X)\r\n",
        "        # 5. Add up all generator and reconstructed losses and perform backprop\r\n",
        "        loss_3 = real_mse_loss(D_Y(out_3))\r\n",
        "        out_4 = G_YtoX(out_3)\r\n",
        "        loss_4 =  cycle_consistency_loss(real_im = images_X, reconstructed_im = out_4, lambda_weight=10)\r\n",
        "\r\n",
        "        g_total_loss = loss_1 + loss_2 + loss_3 + loss_4\r\n",
        "        g_total_loss.backward()\r\n",
        "        g_optimizer.step()\r\n",
        "        \r\n",
        "        # Print the log info\r\n",
        "        if epoch % print_every == 0:\r\n",
        "            # append real and fake discriminator losses and the generator loss\r\n",
        "            losses.append((d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\r\n",
        "            print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f} | d_Y_loss: {:6.4f} | g_total_loss: {:6.4f}'.format(\r\n",
        "                    epoch, n_epochs, d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\r\n",
        "\r\n",
        "            \r\n",
        "        sample_every=100\r\n",
        "        # Save the generated samples\r\n",
        "        if epoch % sample_every == 0:\r\n",
        "            G_YtoX.eval() # set generators to eval mode for sample generation\r\n",
        "            G_XtoY.eval()\r\n",
        "            save_samples(epoch, fixed_Y, fixed_X, G_YtoX, G_XtoY, batch_size=16)\r\n",
        "            G_YtoX.train()\r\n",
        "            G_XtoY.train()\r\n",
        "\r\n",
        "        # uncomment these lines, if you want to save your model\r\n",
        "#         checkpoint_every=1000\r\n",
        "#         # Save the model parameters\r\n",
        "#         if epoch % checkpoint_every == 0:\r\n",
        "#             checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\r\n",
        "\r\n",
        "    return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNYyiltikzxA"
      },
      "source": [
        "n_epochs = 5000 # keep this small when testing if a model first works, then increase it to >=1000\r\n",
        "\r\n",
        "losses = training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN45DEYVpBss"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\r\n",
        "losses = np.array(losses)\r\n",
        "plt.plot(losses.T[0], label='Discriminator, X', alpha=0.5)\r\n",
        "plt.plot(losses.T[1], label='Discriminator, Y', alpha=0.5)\r\n",
        "plt.plot(losses.T[2], label='Generators', alpha=0.5)\r\n",
        "plt.title(\"Training Losses\")\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv2p4wE5pJ-T"
      },
      "source": [
        "import matplotlib.image as mpimg\r\n",
        "\r\n",
        "# helper visualization code\r\n",
        "def view_samples(iteration, sample_dir='samples_cyclegan'):\r\n",
        "    \r\n",
        "    # samples are named by iteration\r\n",
        "    path_XtoY = os.path.join(sample_dir, 'sample-{:06d}-X-Y.png'.format(iteration))\r\n",
        "    path_YtoX = os.path.join(sample_dir, 'sample-{:06d}-Y-X.png'.format(iteration))\r\n",
        "    \r\n",
        "    # read in those samples\r\n",
        "    try: \r\n",
        "        x2y = mpimg.imread(path_XtoY)\r\n",
        "        y2x = mpimg.imread(path_YtoX)\r\n",
        "    except:\r\n",
        "        print('Invalid number of iterations.')\r\n",
        "    \r\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(18,20), nrows=2, ncols=1, sharey=True, sharex=True)\r\n",
        "    ax1.imshow(x2y)\r\n",
        "    ax1.set_title('X to Y')\r\n",
        "    ax2.imshow(y2x)\r\n",
        "    ax2.set_title('Y to X')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dspjRhN1pMsW"
      },
      "source": [
        "# view samples at iteration 100\r\n",
        "view_samples(100, 'samples_cyclegan')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0ayBRvHqy0V"
      },
      "source": [
        "\r\n",
        "# view samples at iteration 1000\r\n",
        "view_samples(1000, 'samples_cyclegan')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7wZxf6_qzz3"
      },
      "source": [
        "view_samples(5000,'samples_cyclegan')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}