{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "There are two csv files that represent student response data for the years 2021 and 2022. The columns of the csv files are:\n",
        "\n",
        "student_id\n",
        "question_id\n",
        "ability - The ability skill of the student.\n",
        "difficulty - The difficulty of the question.\n",
        "answered_correctly - Whether the student answered the question correctly or not\n",
        "Analyse the csv files to answer the following questions:\n",
        "\n",
        "How did the student's ability to answer the questions change ?\n",
        "Did the questions get difficult or easy?\n",
        "Can you create a model that can predict if a student will answer a question correctly?\n",
        "Note down any other observations you may have about the data."
      ],
      "metadata": {
        "id": "V4eXO_qPzqfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "data_2021 = pd.read_csv('student_responses_2021.csv')\n",
        "data_2022 = pd.read_csv('/mnt/data/student_responses_2022.csv')\n",
        "\n",
        "# Display the first few rows and summary info of each file to understand the data structure and content\n",
        "data_2021_info = data_2021.info()\n",
        "data_2021_head = data_2021.head()\n",
        "\n",
        "data_2022_info = data_2022.info()\n",
        "data_2022_head = data_2022.head()\n",
        "\n",
        "data_2021_info, data_2021_head, data_2022_info, data_2022_head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHhfmVGMzrcD",
        "outputId": "d190731c-25c6-4b77-ef8c-e5e7b3bfdc9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 45000 entries, 0 to 44999\n",
            "Data columns (total 5 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   student_id          45000 non-null  int64  \n",
            " 1   question_id         45000 non-null  int64  \n",
            " 2   ability             44100 non-null  float64\n",
            " 3   difficulty          44100 non-null  float64\n",
            " 4   answered_correctly  45000 non-null  bool   \n",
            "dtypes: bool(1), float64(2), int64(2)\n",
            "memory usage: 1.4 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 5 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   student_id          50000 non-null  int64  \n",
            " 1   question_id         50000 non-null  int64  \n",
            " 2   ability             49000 non-null  float64\n",
            " 3   difficulty          49000 non-null  float64\n",
            " 4   answered_correctly  50000 non-null  bool   \n",
            "dtypes: bool(1), float64(2), int64(2)\n",
            "memory usage: 1.6 MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None,\n",
              "    student_id  question_id   ability  difficulty  answered_correctly\n",
              " 0         967           40 -0.320326    1.572793               False\n",
              " 1         135            2 -2.897524   -1.920777               False\n",
              " 2         553           18 -1.207800   -0.482901               False\n",
              " 3         564           29 -2.012802    0.360224               False\n",
              " 4         449           42  0.332158    1.211066               False,\n",
              " None,\n",
              "    student_id  question_id   ability  difficulty  answered_correctly\n",
              " 0        1078           22  1.023237   -0.280792                True\n",
              " 1        1980           37 -2.243018    1.430540               False\n",
              " 2        1240           14  3.453396   -0.872714                True\n",
              " 3        1329           33  1.750230    0.816172                True\n",
              " 4        1995           17  2.288340   -0.558914                True)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations from Summary Statistics\n",
        "Student Ability:\n",
        "\n",
        "In 2021, the mean ability score was approximately -0.042, whereas in 2022, it increased to around 0.33. This suggests an improvement in students' ability scores from 2021 to 2022.\n",
        "The standard deviation also increased from 2.17 in 2021 to 2.45 in 2022, indicating a broader range in student ability scores in 2022.\n",
        "Question Difficulty:\n",
        "\n",
        "The mean difficulty score shifted from -0.057 in 2021 to 0.117 in 2022, suggesting that questions became slightly more challenging on average in 2022.\n",
        "The spread of difficulty scores (indicated by standard deviation) increased slightly from 1.05 to 1.17, meaning there was a wider range of question difficulty levels in 2022.\n",
        "Next, I'll examine how well ability and difficulty predict whether a student will answer a question correctly, using a logistic regression model. ​\n"
      ],
      "metadata": {
        "id": "XLFoUWvJ0eKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate summary statistics for 'ability' and 'difficulty' columns for both years\n",
        "\n",
        "ability_2021_stats = data_2021['ability'].describe()\n",
        "difficulty_2021_stats = data_2021['difficulty'].describe()\n",
        "\n",
        "ability_2022_stats = data_2022['ability'].describe()\n",
        "difficulty_2022_stats = data_2022['difficulty'].describe()\n",
        "\n",
        "ability_2021_stats, difficulty_2021_stats, ability_2022_stats, difficulty_2022_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IA84VU00cwk",
        "outputId": "a9aa83ed-60b3-415c-d65f-4914fabcdf48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(count    44100.000000\n",
              " mean        -0.041824\n",
              " std          2.167235\n",
              " min         -5.751698\n",
              " 25%         -1.507034\n",
              " 50%         -0.068255\n",
              " 75%          1.442855\n",
              " max          5.889554\n",
              " Name: ability, dtype: float64,\n",
              " count    44100.000000\n",
              " mean        -0.057131\n",
              " std          1.053968\n",
              " min         -2.157522\n",
              " 25%         -0.870725\n",
              " 50%         -0.147794\n",
              " 75%          0.904278\n",
              " max          1.774446\n",
              " Name: difficulty, dtype: float64,\n",
              " count    49000.000000\n",
              " mean         0.330433\n",
              " std          2.448007\n",
              " min         -5.653869\n",
              " 25%         -1.047968\n",
              " 50%          0.101803\n",
              " 75%          1.704314\n",
              " max          9.957734\n",
              " Name: ability, dtype: float64,\n",
              " count    49000.000000\n",
              " mean         0.116709\n",
              " std          1.171172\n",
              " min         -1.955096\n",
              " 25%         -0.728539\n",
              " 50%         -0.007172\n",
              " 75%          1.199790\n",
              " max          2.194765\n",
              " Name: difficulty, dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a model that predicts if a student will answer a question correctly based on attributes like the student's ability and question difficulty, we’ll go through the following steps:\n",
        "\n",
        "Data Preparation: Load and preprocess the data.\n",
        "Feature Engineering: Create features that capture relevant information.\n",
        "Model Selection and Training: Train a model to predict the probability of a correct answer.\n",
        "Evaluation: Evaluate model performance on a test set."
      ],
      "metadata": {
        "id": "8PIcUMgXEF4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate both datasets for a larger training set\n",
        "data = pd.concat([data_2021, data_2022], ignore_index=True)\n",
        "\n",
        "# Check for missing values\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Drop rows with missing values, if any\n",
        "data.dropna(inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZjTkEV70xEE",
        "outputId": "e6ce507c-84b9-4f1b-807b-3b84e436557c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "student_id               0\n",
            "question_id              0\n",
            "ability               1900\n",
            "difficulty            1900\n",
            "answered_correctly       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Feature Engineering\n",
        "We’ll create new features or modify existing ones. Features such as ability and difficulty may be scaled for improved performance."
      ],
      "metadata": {
        "id": "Plw9bpEDEewA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select features and target variable\n",
        "X = data[['ability', 'difficulty']]\n",
        "y = data['answered_correctly']\n",
        "\n",
        "# Scale features for better model performance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "3NkLIYIJEN17"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Selection and Training\n",
        "We’ll use a simple model, such as Logistic Regression, to start with. Logistic Regression is suitable as it can output the probability of a student answering correctly."
      ],
      "metadata": {
        "id": "q2laXkf-Er5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n"
      ],
      "metadata": {
        "id": "W-fO1_-CEoDQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        "Evaluate the model with metrics such as accuracy and AUC-ROC, which are helpful for binary classification."
      ],
      "metadata": {
        "id": "wSNo4BLcEyMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Displaying results\n",
        "accuracy, roc_auc, conf_matrix\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rUnzmgSEvLk",
        "outputId": "7cc1e5d8-c11e-4296-c08b-3bf68a576fa2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9996164383561644,\n",
              " 0.9999999519201118,\n",
              " array([[8852,    7],\n",
              "        [   0, 9391]]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9LV7duRTE1ht"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}