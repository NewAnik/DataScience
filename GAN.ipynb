{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPWie+r3OnYy6EMVZsawETp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NewAnik/DataScience/blob/master/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21EQDu5Skk5y"
      },
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "import torchvision.datasets as datasets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "#mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxxj_Dbz1EV1"
      },
      "source": [
        "#mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kklxLC572q6T"
      },
      "source": [
        "# 1\r\n",
        "num_workers = 0\r\n",
        "# 2\r\n",
        "batch_size = 64\r\n",
        "# 3\r\n",
        "transform = transforms.ToTensor()\r\n",
        "# 4\r\n",
        "train_data = datasets.MNIST(root='data', train=True,\r\n",
        " download=True, transform=transform)\r\n",
        "# 5\r\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\r\n",
        " num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndDdGQIn1LC1",
        "outputId": "930360fc-585e-45f2-9c01-819f456302b7"
      },
      "source": [
        "len(train_loader)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "U9LDihdxPbP0",
        "outputId": "140e27ca-169f-41ef-add5-c80c4ed77d4d"
      },
      "source": [
        "#1 \r\n",
        "dataiter = iter(train_loader)\r\n",
        "images, labels = dataiter.next()\r\n",
        "images = images.numpy()\r\n",
        "#2\r\n",
        "img = np.squeeze(images[0])\r\n",
        "fig = plt.figure(figsize = (3,3)) \r\n",
        "ax = fig.add_subplot(111)\r\n",
        "ax.imshow(img, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0848d1e588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnt-h0a0QHAG"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "class Discriminator(nn.Module):\r\n",
        "  def __init__(self, input_size, hidden_dim, output_size):\r\n",
        "    super(Discriminator, self).__init__()\r\n",
        " \r\n",
        " # 1\r\n",
        "    self.fc1 = nn.Linear(input_size, hidden_dim*4)\r\n",
        "    self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\r\n",
        "    self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\r\n",
        " \r\n",
        " # 2\r\n",
        "    self.fc4 = nn.Linear(hidden_dim, output_size)\r\n",
        " \r\n",
        " # dropout layer \r\n",
        "    self.dropout = nn.Dropout(0.3)\r\n",
        " \r\n",
        " \r\n",
        "  def forward(self, x):\r\n",
        " #3 \r\n",
        "    x = x.view(-1, 28*28)\r\n",
        " #4 \r\n",
        "    x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\r\n",
        "    x = self.dropout(x)\r\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2)\r\n",
        "    x = self.dropout(x)\r\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2)\r\n",
        "    x = self.dropout(x)\r\n",
        " # 5\r\n",
        "    out = self.fc4(x)\r\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M7UIPoPhgHh"
      },
      "source": [
        "class Generator(nn.Module):\r\n",
        "  def __init__(self, input_size, hidden_dim, output_size):\r\n",
        "    super(Generator, self).__init__()\r\n",
        " \r\n",
        " # 1\r\n",
        "    self.fc1 = nn.Linear(input_size, hidden_dim)\r\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\r\n",
        "    self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\r\n",
        " \r\n",
        " # 2\r\n",
        "    self.fc4 = nn.Linear(hidden_dim*4, output_size)\r\n",
        " \r\n",
        " # 3\r\n",
        "    self.dropout = nn.Dropout(0.3)\r\n",
        "  def forward(self, x):\r\n",
        " # 4\r\n",
        "    x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\r\n",
        "    x = self.dropout(x)\r\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2)\r\n",
        "    x = self.dropout(x)\r\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2)\r\n",
        "    x = self.dropout(x)\r\n",
        " # 5\r\n",
        "    out = F.tanh(self.fc4(x))\r\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfwbKC-fkdBz"
      },
      "source": [
        "# Discriminator hyperparameters\r\n",
        "# 1\r\n",
        "input_size = 784\r\n",
        "# 2\r\n",
        "d_output_size = 1\r\n",
        "# 3\r\n",
        "d_hidden_size = 32\r\n",
        "# Generator hyperparams\r\n",
        "# 4\r\n",
        "z_size = 100\r\n",
        "# 5\r\n",
        "g_output_size = 784\r\n",
        "# 6\r\n",
        "g_hidden_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6cW8Ip_lWR2",
        "outputId": "1f21d0c3-20c8-4259-a1d9-034d9348c19d"
      },
      "source": [
        "# instantiate discriminator and generator\r\n",
        "D = Discriminator(input_size, d_hidden_size, d_output_size)\r\n",
        "G = Generator(z_size, g_hidden_size, g_output_size)\r\n",
        "# check that they are as you expect\r\n",
        "print(D)\r\n",
        "print()\r\n",
        "print(G)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n",
            "\n",
            "Generator(\n",
            "  (fc1): Linear(in_features=100, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=784, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psD1MX30rpbL"
      },
      "source": [
        "# Calculate losses\r\n",
        "def real_loss(D_out, smooth=False):\r\n",
        " batch_size = D_out.size(0)\r\n",
        " # label smoothing\r\n",
        " if smooth:\r\n",
        " # smooth, real labels = 0.9\r\n",
        "  labels = torch.ones(batch_size)*0.9\r\n",
        " else:\r\n",
        "  labels = torch.ones(batch_size) # real labels = 1\r\n",
        " \r\n",
        " # numerically stable loss\r\n",
        " criterion = nn.BCEWithLogitsLoss()\r\n",
        " # calculate loss\r\n",
        " loss = criterion(D_out.squeeze(), labels)\r\n",
        " return loss\r\n",
        "def fake_loss(D_out):\r\n",
        " batch_size = D_out.size(0)\r\n",
        " labels = torch.zeros(batch_size) # fake labels = 0\r\n",
        " criterion = nn.BCEWithLogitsLoss()\r\n",
        " # calculate loss\r\n",
        " loss = criterion(D_out.squeeze(), labels)\r\n",
        " return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J7xJsDer2gC"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "lr = 0.002\r\n",
        "d_optimizer = optim.Adam(D.parameters(), lr)\r\n",
        "g_optimizer = optim.Adam(G.parameters(), lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skJB_JlPtyy4"
      },
      "source": [
        "# Calculate losses\r\n",
        "def real_loss(D_out, smooth=False):\r\n",
        "    batch_size = D_out.size(0)\r\n",
        "    # label smoothing\r\n",
        "    if smooth:\r\n",
        "        # smooth, real labels = 0.9\r\n",
        "        labels = torch.ones(batch_size)*0.9\r\n",
        "    else:\r\n",
        "        labels = torch.ones(batch_size) # real labels = 1\r\n",
        "        \r\n",
        "    # numerically stable loss\r\n",
        "    criterion = nn.BCEWithLogitsLoss()\r\n",
        "    # calculate loss\r\n",
        "    loss = criterion(D_out.squeeze(), labels)\r\n",
        "    return loss\r\n",
        "\r\n",
        "def fake_loss(D_out):\r\n",
        "    batch_size = D_out.size(0)\r\n",
        "    labels = torch.zeros(batch_size) # fake labels = 0\r\n",
        "    criterion = nn.BCEWithLogitsLoss()\r\n",
        "    # calculate loss\r\n",
        "    loss = criterion(D_out.squeeze(), labels)\r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjOK-eATt34x"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "# Optimizers\r\n",
        "lr = 0.002\r\n",
        "\r\n",
        "# Create optimizers for the discriminator and generator\r\n",
        "d_optimizer = optim.Adam(D.parameters(), lr)\r\n",
        "g_optimizer = optim.Adam(G.parameters(), lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNqyLgw2t-jS",
        "outputId": "bc68eb4e-95c7-4c3d-9517-6eea0455a604"
      },
      "source": [
        "import pickle as pkl\r\n",
        "\r\n",
        "# training hyperparams\r\n",
        "num_epochs = 100\r\n",
        "\r\n",
        "# keep track of loss and generated, \"fake\" samples\r\n",
        "samples = []\r\n",
        "losses = []\r\n",
        "\r\n",
        "print_every = 400\r\n",
        "\r\n",
        "# Get some fixed data for sampling. These are images that are held\r\n",
        "# constant throughout training, and allow us to inspect the model's performance\r\n",
        "sample_size=16\r\n",
        "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\r\n",
        "fixed_z = torch.from_numpy(fixed_z).float()\r\n",
        "\r\n",
        "# train the network\r\n",
        "D.train()\r\n",
        "G.train()\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    \r\n",
        "    for batch_i, (real_images, _) in enumerate(train_loader):\r\n",
        "                \r\n",
        "        batch_size = real_images.size(0)\r\n",
        "        \r\n",
        "        ## Important rescaling step ## \r\n",
        "        real_images = real_images*2 - 1  # rescale input images from [0,1) to [-1, 1)\r\n",
        "        \r\n",
        "        # ============================================\r\n",
        "        #            TRAIN THE DISCRIMINATOR\r\n",
        "        # ============================================\r\n",
        "        \r\n",
        "        d_optimizer.zero_grad()\r\n",
        "        \r\n",
        "        # 1. Train with real images\r\n",
        "\r\n",
        "        # Compute the discriminator losses on real images \r\n",
        "        # smooth the real labels\r\n",
        "        D_real = D(real_images)\r\n",
        "        d_real_loss = real_loss(D_real, smooth=True)\r\n",
        "        \r\n",
        "        # 2. Train with fake images\r\n",
        "        \r\n",
        "        # Generate fake images\r\n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\r\n",
        "        z = torch.from_numpy(z).float()\r\n",
        "        fake_images = G(z)\r\n",
        "        \r\n",
        "        # Compute the discriminator losses on fake images        \r\n",
        "        D_fake = D(fake_images)\r\n",
        "        d_fake_loss = fake_loss(D_fake)\r\n",
        "        \r\n",
        "        # add up loss and perform backprop\r\n",
        "        d_loss = d_real_loss + d_fake_loss\r\n",
        "        d_loss.backward()\r\n",
        "        d_optimizer.step()\r\n",
        "        \r\n",
        "        \r\n",
        "        # =========================================\r\n",
        "        #            TRAIN THE GENERATOR\r\n",
        "        # =========================================\r\n",
        "        g_optimizer.zero_grad()\r\n",
        "        \r\n",
        "        # 1. Train with fake images and flipped labels\r\n",
        "        \r\n",
        "        # Generate fake images\r\n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\r\n",
        "        z = torch.from_numpy(z).float()\r\n",
        "        fake_images = G(z)\r\n",
        "        \r\n",
        "        # Compute the discriminator losses on fake images \r\n",
        "        # using flipped labels!\r\n",
        "        D_fake = D(fake_images)\r\n",
        "        g_loss = real_loss(D_fake) # use real loss to flip labels\r\n",
        "        \r\n",
        "        # perform backprop\r\n",
        "        g_loss.backward()\r\n",
        "        g_optimizer.step()\r\n",
        "\r\n",
        "        # Print some loss stats\r\n",
        "        if batch_i % print_every == 0:\r\n",
        "            # print discriminator and generator loss\r\n",
        "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\r\n",
        "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\r\n",
        "\r\n",
        "    \r\n",
        "    ## AFTER EACH EPOCH##\r\n",
        "    # append discriminator loss and generator loss\r\n",
        "    losses.append((d_loss.item(), g_loss.item()))\r\n",
        "    \r\n",
        "    # generate and save sample, fake images\r\n",
        "    G.eval() # eval mode for generating samples\r\n",
        "    samples_z = G(fixed_z)\r\n",
        "    samples.append(samples_z)\r\n",
        "    G.train() # back to train mode\r\n",
        "\r\n",
        "\r\n",
        "# Save training generator samples\r\n",
        "with open('train_samples.pkl', 'wb') as f:\r\n",
        "    pkl.dump(samples, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [    1/  100] | d_loss: 1.3786 | g_loss: 0.7076\n",
            "Epoch [    1/  100] | d_loss: 1.0481 | g_loss: 4.0675\n",
            "Epoch [    1/  100] | d_loss: 1.0536 | g_loss: 1.2011\n",
            "Epoch [    2/  100] | d_loss: 1.6421 | g_loss: 0.6373\n",
            "Epoch [    2/  100] | d_loss: 0.8308 | g_loss: 1.7854\n",
            "Epoch [    2/  100] | d_loss: 1.3120 | g_loss: 0.8232\n",
            "Epoch [    3/  100] | d_loss: 1.2293 | g_loss: 0.7817\n",
            "Epoch [    3/  100] | d_loss: 1.1336 | g_loss: 1.1842\n",
            "Epoch [    3/  100] | d_loss: 1.4989 | g_loss: 0.6847\n",
            "Epoch [    4/  100] | d_loss: 1.0473 | g_loss: 2.1237\n",
            "Epoch [    4/  100] | d_loss: 1.2155 | g_loss: 1.1755\n",
            "Epoch [    4/  100] | d_loss: 1.4246 | g_loss: 1.0132\n",
            "Epoch [    5/  100] | d_loss: 1.2628 | g_loss: 0.7201\n",
            "Epoch [    5/  100] | d_loss: 1.0843 | g_loss: 1.3223\n",
            "Epoch [    5/  100] | d_loss: 1.0803 | g_loss: 1.1718\n",
            "Epoch [    6/  100] | d_loss: 1.2275 | g_loss: 1.2290\n",
            "Epoch [    6/  100] | d_loss: 0.9920 | g_loss: 1.2678\n",
            "Epoch [    6/  100] | d_loss: 1.3000 | g_loss: 0.9269\n",
            "Epoch [    7/  100] | d_loss: 1.4335 | g_loss: 1.4521\n",
            "Epoch [    7/  100] | d_loss: 1.3188 | g_loss: 0.8870\n",
            "Epoch [    7/  100] | d_loss: 1.1888 | g_loss: 1.1398\n",
            "Epoch [    8/  100] | d_loss: 1.3938 | g_loss: 2.0463\n",
            "Epoch [    8/  100] | d_loss: 1.0059 | g_loss: 1.7582\n",
            "Epoch [    8/  100] | d_loss: 1.0759 | g_loss: 0.9445\n",
            "Epoch [    9/  100] | d_loss: 1.1666 | g_loss: 1.0216\n",
            "Epoch [    9/  100] | d_loss: 1.1735 | g_loss: 1.5850\n",
            "Epoch [    9/  100] | d_loss: 1.3416 | g_loss: 0.8468\n",
            "Epoch [   10/  100] | d_loss: 1.2783 | g_loss: 1.0656\n",
            "Epoch [   10/  100] | d_loss: 1.2417 | g_loss: 1.2703\n",
            "Epoch [   10/  100] | d_loss: 1.1090 | g_loss: 1.2850\n",
            "Epoch [   11/  100] | d_loss: 1.2040 | g_loss: 1.1119\n",
            "Epoch [   11/  100] | d_loss: 1.1688 | g_loss: 1.4721\n",
            "Epoch [   11/  100] | d_loss: 1.1671 | g_loss: 1.0376\n",
            "Epoch [   12/  100] | d_loss: 1.2696 | g_loss: 0.9956\n",
            "Epoch [   12/  100] | d_loss: 1.3051 | g_loss: 0.8361\n",
            "Epoch [   12/  100] | d_loss: 1.2475 | g_loss: 1.1012\n",
            "Epoch [   13/  100] | d_loss: 1.2272 | g_loss: 1.3271\n",
            "Epoch [   13/  100] | d_loss: 1.2931 | g_loss: 1.4873\n",
            "Epoch [   13/  100] | d_loss: 1.3173 | g_loss: 0.8799\n",
            "Epoch [   14/  100] | d_loss: 1.2849 | g_loss: 1.2977\n",
            "Epoch [   14/  100] | d_loss: 1.0732 | g_loss: 1.2650\n",
            "Epoch [   14/  100] | d_loss: 1.3137 | g_loss: 1.3088\n",
            "Epoch [   15/  100] | d_loss: 1.3192 | g_loss: 0.9567\n",
            "Epoch [   15/  100] | d_loss: 1.2516 | g_loss: 1.2103\n",
            "Epoch [   15/  100] | d_loss: 1.2375 | g_loss: 1.0823\n",
            "Epoch [   16/  100] | d_loss: 1.2198 | g_loss: 1.0796\n",
            "Epoch [   16/  100] | d_loss: 1.2056 | g_loss: 0.8530\n",
            "Epoch [   16/  100] | d_loss: 1.2944 | g_loss: 1.1658\n",
            "Epoch [   17/  100] | d_loss: 1.1512 | g_loss: 1.1353\n",
            "Epoch [   17/  100] | d_loss: 1.2002 | g_loss: 1.0587\n",
            "Epoch [   17/  100] | d_loss: 1.2932 | g_loss: 1.0491\n",
            "Epoch [   18/  100] | d_loss: 1.4144 | g_loss: 1.2195\n",
            "Epoch [   18/  100] | d_loss: 1.2821 | g_loss: 1.0149\n",
            "Epoch [   18/  100] | d_loss: 1.5041 | g_loss: 0.8738\n",
            "Epoch [   19/  100] | d_loss: 1.4042 | g_loss: 1.1020\n",
            "Epoch [   19/  100] | d_loss: 1.2975 | g_loss: 0.9283\n",
            "Epoch [   19/  100] | d_loss: 1.3344 | g_loss: 1.1460\n",
            "Epoch [   20/  100] | d_loss: 1.3296 | g_loss: 0.9722\n",
            "Epoch [   20/  100] | d_loss: 1.2427 | g_loss: 1.0864\n",
            "Epoch [   20/  100] | d_loss: 1.3148 | g_loss: 1.1238\n",
            "Epoch [   21/  100] | d_loss: 1.1654 | g_loss: 1.2671\n",
            "Epoch [   21/  100] | d_loss: 1.1721 | g_loss: 1.5679\n",
            "Epoch [   21/  100] | d_loss: 1.3461 | g_loss: 0.9401\n",
            "Epoch [   22/  100] | d_loss: 1.3820 | g_loss: 1.2400\n",
            "Epoch [   22/  100] | d_loss: 1.3338 | g_loss: 1.0073\n",
            "Epoch [   22/  100] | d_loss: 1.2294 | g_loss: 1.0326\n",
            "Epoch [   23/  100] | d_loss: 1.1676 | g_loss: 0.8733\n",
            "Epoch [   23/  100] | d_loss: 1.1838 | g_loss: 1.2516\n",
            "Epoch [   23/  100] | d_loss: 1.2372 | g_loss: 1.2465\n",
            "Epoch [   24/  100] | d_loss: 1.2211 | g_loss: 1.0950\n",
            "Epoch [   24/  100] | d_loss: 1.2974 | g_loss: 0.8669\n",
            "Epoch [   24/  100] | d_loss: 1.4002 | g_loss: 1.1504\n",
            "Epoch [   25/  100] | d_loss: 1.3156 | g_loss: 1.0037\n",
            "Epoch [   25/  100] | d_loss: 1.2937 | g_loss: 1.0077\n",
            "Epoch [   25/  100] | d_loss: 1.3848 | g_loss: 0.9397\n",
            "Epoch [   26/  100] | d_loss: 1.3797 | g_loss: 1.0038\n",
            "Epoch [   26/  100] | d_loss: 1.3230 | g_loss: 0.9307\n",
            "Epoch [   26/  100] | d_loss: 1.3793 | g_loss: 1.1187\n",
            "Epoch [   27/  100] | d_loss: 1.3058 | g_loss: 0.9720\n",
            "Epoch [   27/  100] | d_loss: 1.2519 | g_loss: 0.9993\n",
            "Epoch [   27/  100] | d_loss: 1.3848 | g_loss: 0.9206\n",
            "Epoch [   28/  100] | d_loss: 1.2727 | g_loss: 1.2812\n",
            "Epoch [   28/  100] | d_loss: 1.1741 | g_loss: 1.3286\n",
            "Epoch [   28/  100] | d_loss: 1.3599 | g_loss: 0.9118\n",
            "Epoch [   29/  100] | d_loss: 1.2460 | g_loss: 1.3815\n",
            "Epoch [   29/  100] | d_loss: 1.1964 | g_loss: 1.5301\n",
            "Epoch [   29/  100] | d_loss: 1.2843 | g_loss: 1.0901\n",
            "Epoch [   30/  100] | d_loss: 1.3145 | g_loss: 0.9559\n",
            "Epoch [   30/  100] | d_loss: 1.2783 | g_loss: 1.0218\n",
            "Epoch [   30/  100] | d_loss: 1.3199 | g_loss: 1.3529\n",
            "Epoch [   31/  100] | d_loss: 1.3659 | g_loss: 0.8441\n",
            "Epoch [   31/  100] | d_loss: 1.1790 | g_loss: 1.2368\n",
            "Epoch [   31/  100] | d_loss: 1.3208 | g_loss: 1.0577\n",
            "Epoch [   32/  100] | d_loss: 1.2864 | g_loss: 1.1028\n",
            "Epoch [   32/  100] | d_loss: 1.2583 | g_loss: 0.9987\n",
            "Epoch [   32/  100] | d_loss: 1.2638 | g_loss: 1.1601\n",
            "Epoch [   33/  100] | d_loss: 1.3736 | g_loss: 1.0001\n",
            "Epoch [   33/  100] | d_loss: 1.3215 | g_loss: 1.2156\n",
            "Epoch [   33/  100] | d_loss: 1.3417 | g_loss: 0.9681\n",
            "Epoch [   34/  100] | d_loss: 1.1468 | g_loss: 1.4436\n",
            "Epoch [   34/  100] | d_loss: 1.2098 | g_loss: 0.9282\n",
            "Epoch [   34/  100] | d_loss: 1.2112 | g_loss: 1.8352\n",
            "Epoch [   35/  100] | d_loss: 1.3708 | g_loss: 1.1664\n",
            "Epoch [   35/  100] | d_loss: 1.1874 | g_loss: 0.9430\n",
            "Epoch [   35/  100] | d_loss: 1.2529 | g_loss: 1.8448\n",
            "Epoch [   36/  100] | d_loss: 1.3135 | g_loss: 0.8346\n",
            "Epoch [   36/  100] | d_loss: 1.2523 | g_loss: 0.9295\n",
            "Epoch [   36/  100] | d_loss: 1.3228 | g_loss: 0.9567\n",
            "Epoch [   37/  100] | d_loss: 1.2644 | g_loss: 0.9138\n",
            "Epoch [   37/  100] | d_loss: 1.2814 | g_loss: 1.1572\n",
            "Epoch [   37/  100] | d_loss: 1.2633 | g_loss: 1.0196\n",
            "Epoch [   38/  100] | d_loss: 1.3431 | g_loss: 1.5890\n",
            "Epoch [   38/  100] | d_loss: 1.2071 | g_loss: 1.3642\n",
            "Epoch [   38/  100] | d_loss: 1.4002 | g_loss: 1.0481\n",
            "Epoch [   39/  100] | d_loss: 1.3927 | g_loss: 1.1577\n",
            "Epoch [   39/  100] | d_loss: 1.2538 | g_loss: 1.0257\n",
            "Epoch [   39/  100] | d_loss: 1.2095 | g_loss: 0.8703\n",
            "Epoch [   40/  100] | d_loss: 1.3202 | g_loss: 0.9205\n",
            "Epoch [   40/  100] | d_loss: 1.2507 | g_loss: 0.8798\n",
            "Epoch [   40/  100] | d_loss: 1.4833 | g_loss: 1.0244\n",
            "Epoch [   41/  100] | d_loss: 1.2566 | g_loss: 0.8857\n",
            "Epoch [   41/  100] | d_loss: 1.2995 | g_loss: 0.9258\n",
            "Epoch [   41/  100] | d_loss: 1.2839 | g_loss: 0.8262\n",
            "Epoch [   42/  100] | d_loss: 1.2832 | g_loss: 0.9425\n",
            "Epoch [   42/  100] | d_loss: 1.3253 | g_loss: 0.8707\n",
            "Epoch [   42/  100] | d_loss: 1.2712 | g_loss: 1.1101\n",
            "Epoch [   43/  100] | d_loss: 1.3016 | g_loss: 1.1028\n",
            "Epoch [   43/  100] | d_loss: 1.3507 | g_loss: 0.9416\n",
            "Epoch [   43/  100] | d_loss: 1.3102 | g_loss: 0.9764\n",
            "Epoch [   44/  100] | d_loss: 1.3677 | g_loss: 1.0521\n",
            "Epoch [   44/  100] | d_loss: 1.4062 | g_loss: 0.9343\n",
            "Epoch [   44/  100] | d_loss: 1.2966 | g_loss: 1.0221\n",
            "Epoch [   45/  100] | d_loss: 1.1726 | g_loss: 1.8513\n",
            "Epoch [   45/  100] | d_loss: 1.2985 | g_loss: 0.9648\n",
            "Epoch [   45/  100] | d_loss: 1.2802 | g_loss: 1.2581\n",
            "Epoch [   46/  100] | d_loss: 1.3351 | g_loss: 1.0417\n",
            "Epoch [   46/  100] | d_loss: 1.2921 | g_loss: 1.0301\n",
            "Epoch [   46/  100] | d_loss: 1.2360 | g_loss: 1.0589\n",
            "Epoch [   47/  100] | d_loss: 1.3743 | g_loss: 1.0541\n",
            "Epoch [   47/  100] | d_loss: 1.3142 | g_loss: 0.8382\n",
            "Epoch [   47/  100] | d_loss: 1.3598 | g_loss: 0.9791\n",
            "Epoch [   48/  100] | d_loss: 1.2937 | g_loss: 0.9063\n",
            "Epoch [   48/  100] | d_loss: 1.2240 | g_loss: 0.8706\n",
            "Epoch [   48/  100] | d_loss: 1.3400 | g_loss: 0.9429\n",
            "Epoch [   49/  100] | d_loss: 1.2813 | g_loss: 1.1853\n",
            "Epoch [   49/  100] | d_loss: 1.2742 | g_loss: 0.9923\n",
            "Epoch [   49/  100] | d_loss: 1.3886 | g_loss: 0.9847\n",
            "Epoch [   50/  100] | d_loss: 1.2965 | g_loss: 0.8937\n",
            "Epoch [   50/  100] | d_loss: 1.3520 | g_loss: 1.0028\n",
            "Epoch [   50/  100] | d_loss: 1.2879 | g_loss: 1.1834\n",
            "Epoch [   51/  100] | d_loss: 1.3312 | g_loss: 1.2250\n",
            "Epoch [   51/  100] | d_loss: 1.2802 | g_loss: 0.8979\n",
            "Epoch [   51/  100] | d_loss: 1.3005 | g_loss: 1.0964\n",
            "Epoch [   52/  100] | d_loss: 1.2634 | g_loss: 0.8684\n",
            "Epoch [   52/  100] | d_loss: 1.2780 | g_loss: 0.9014\n",
            "Epoch [   52/  100] | d_loss: 1.2565 | g_loss: 1.0162\n",
            "Epoch [   53/  100] | d_loss: 1.3210 | g_loss: 0.9329\n",
            "Epoch [   53/  100] | d_loss: 1.3288 | g_loss: 1.0880\n",
            "Epoch [   53/  100] | d_loss: 1.2988 | g_loss: 0.8809\n",
            "Epoch [   54/  100] | d_loss: 1.2649 | g_loss: 0.8707\n",
            "Epoch [   54/  100] | d_loss: 1.3555 | g_loss: 0.9031\n",
            "Epoch [   54/  100] | d_loss: 1.3376 | g_loss: 0.9938\n",
            "Epoch [   55/  100] | d_loss: 1.2859 | g_loss: 1.0671\n",
            "Epoch [   55/  100] | d_loss: 1.3469 | g_loss: 1.1156\n",
            "Epoch [   55/  100] | d_loss: 1.1986 | g_loss: 1.0495\n",
            "Epoch [   56/  100] | d_loss: 1.2612 | g_loss: 0.9822\n",
            "Epoch [   56/  100] | d_loss: 1.4082 | g_loss: 0.9557\n",
            "Epoch [   56/  100] | d_loss: 1.3027 | g_loss: 0.9155\n",
            "Epoch [   57/  100] | d_loss: 1.2876 | g_loss: 0.7302\n",
            "Epoch [   57/  100] | d_loss: 1.3105 | g_loss: 0.9239\n",
            "Epoch [   57/  100] | d_loss: 1.3134 | g_loss: 0.8866\n",
            "Epoch [   58/  100] | d_loss: 1.2927 | g_loss: 0.9129\n",
            "Epoch [   58/  100] | d_loss: 1.3225 | g_loss: 0.9271\n",
            "Epoch [   58/  100] | d_loss: 1.2860 | g_loss: 1.1333\n",
            "Epoch [   59/  100] | d_loss: 1.2321 | g_loss: 1.2683\n",
            "Epoch [   59/  100] | d_loss: 1.3563 | g_loss: 0.9687\n",
            "Epoch [   59/  100] | d_loss: 1.3186 | g_loss: 0.9243\n",
            "Epoch [   60/  100] | d_loss: 1.2862 | g_loss: 1.2574\n",
            "Epoch [   60/  100] | d_loss: 1.2888 | g_loss: 1.1229\n",
            "Epoch [   60/  100] | d_loss: 1.3257 | g_loss: 1.2338\n",
            "Epoch [   61/  100] | d_loss: 1.3073 | g_loss: 0.7845\n",
            "Epoch [   61/  100] | d_loss: 1.2395 | g_loss: 0.8995\n",
            "Epoch [   61/  100] | d_loss: 1.2915 | g_loss: 1.0343\n",
            "Epoch [   62/  100] | d_loss: 1.3151 | g_loss: 0.8563\n",
            "Epoch [   62/  100] | d_loss: 1.1765 | g_loss: 1.4111\n",
            "Epoch [   62/  100] | d_loss: 1.2941 | g_loss: 0.9370\n",
            "Epoch [   63/  100] | d_loss: 1.3328 | g_loss: 1.2393\n",
            "Epoch [   63/  100] | d_loss: 1.2578 | g_loss: 1.0563\n",
            "Epoch [   63/  100] | d_loss: 1.3713 | g_loss: 1.1279\n",
            "Epoch [   64/  100] | d_loss: 1.3251 | g_loss: 0.9578\n",
            "Epoch [   64/  100] | d_loss: 1.3104 | g_loss: 0.9681\n",
            "Epoch [   64/  100] | d_loss: 1.3873 | g_loss: 1.0273\n",
            "Epoch [   65/  100] | d_loss: 1.2107 | g_loss: 1.2001\n",
            "Epoch [   65/  100] | d_loss: 1.2926 | g_loss: 0.9379\n",
            "Epoch [   65/  100] | d_loss: 1.3779 | g_loss: 1.0085\n",
            "Epoch [   66/  100] | d_loss: 1.2547 | g_loss: 0.7889\n",
            "Epoch [   66/  100] | d_loss: 1.3562 | g_loss: 0.9473\n",
            "Epoch [   66/  100] | d_loss: 1.3287 | g_loss: 1.0650\n",
            "Epoch [   67/  100] | d_loss: 1.3060 | g_loss: 0.9274\n",
            "Epoch [   67/  100] | d_loss: 1.2058 | g_loss: 0.9615\n",
            "Epoch [   67/  100] | d_loss: 1.2813 | g_loss: 1.0160\n",
            "Epoch [   68/  100] | d_loss: 1.1952 | g_loss: 1.0961\n",
            "Epoch [   68/  100] | d_loss: 1.3588 | g_loss: 0.8722\n",
            "Epoch [   68/  100] | d_loss: 1.2766 | g_loss: 1.0091\n",
            "Epoch [   69/  100] | d_loss: 1.2896 | g_loss: 0.9327\n",
            "Epoch [   69/  100] | d_loss: 1.2834 | g_loss: 1.0433\n",
            "Epoch [   69/  100] | d_loss: 1.2062 | g_loss: 1.4592\n",
            "Epoch [   70/  100] | d_loss: 1.2632 | g_loss: 1.0705\n",
            "Epoch [   70/  100] | d_loss: 1.1906 | g_loss: 0.9469\n",
            "Epoch [   70/  100] | d_loss: 1.4304 | g_loss: 1.2460\n",
            "Epoch [   71/  100] | d_loss: 1.4139 | g_loss: 0.7859\n",
            "Epoch [   71/  100] | d_loss: 1.2393 | g_loss: 0.8378\n",
            "Epoch [   71/  100] | d_loss: 1.2375 | g_loss: 1.2058\n",
            "Epoch [   72/  100] | d_loss: 1.2222 | g_loss: 0.8676\n",
            "Epoch [   72/  100] | d_loss: 1.2282 | g_loss: 1.0519\n",
            "Epoch [   72/  100] | d_loss: 1.3241 | g_loss: 1.1851\n",
            "Epoch [   73/  100] | d_loss: 1.1979 | g_loss: 1.1556\n",
            "Epoch [   73/  100] | d_loss: 1.3608 | g_loss: 1.1317\n",
            "Epoch [   73/  100] | d_loss: 1.2997 | g_loss: 0.8890\n",
            "Epoch [   74/  100] | d_loss: 1.3429 | g_loss: 1.1204\n",
            "Epoch [   74/  100] | d_loss: 1.1842 | g_loss: 1.1129\n",
            "Epoch [   74/  100] | d_loss: 1.2836 | g_loss: 1.0505\n",
            "Epoch [   75/  100] | d_loss: 1.3013 | g_loss: 0.9846\n",
            "Epoch [   75/  100] | d_loss: 1.2524 | g_loss: 1.0261\n",
            "Epoch [   75/  100] | d_loss: 1.3750 | g_loss: 0.9915\n",
            "Epoch [   76/  100] | d_loss: 1.3534 | g_loss: 1.1119\n",
            "Epoch [   76/  100] | d_loss: 1.3217 | g_loss: 0.9453\n",
            "Epoch [   76/  100] | d_loss: 1.3094 | g_loss: 0.8340\n",
            "Epoch [   77/  100] | d_loss: 1.2709 | g_loss: 1.2138\n",
            "Epoch [   77/  100] | d_loss: 1.2872 | g_loss: 0.9946\n",
            "Epoch [   77/  100] | d_loss: 1.3725 | g_loss: 1.0864\n",
            "Epoch [   78/  100] | d_loss: 1.2448 | g_loss: 1.0981\n",
            "Epoch [   78/  100] | d_loss: 1.2746 | g_loss: 1.0609\n",
            "Epoch [   78/  100] | d_loss: 1.3123 | g_loss: 1.1058\n",
            "Epoch [   79/  100] | d_loss: 1.2586 | g_loss: 1.0323\n",
            "Epoch [   79/  100] | d_loss: 1.2518 | g_loss: 0.9418\n",
            "Epoch [   79/  100] | d_loss: 1.3558 | g_loss: 1.0653\n",
            "Epoch [   80/  100] | d_loss: 1.3385 | g_loss: 0.9117\n",
            "Epoch [   80/  100] | d_loss: 1.2570 | g_loss: 1.1260\n",
            "Epoch [   80/  100] | d_loss: 1.2763 | g_loss: 1.0646\n",
            "Epoch [   81/  100] | d_loss: 1.2959 | g_loss: 0.9322\n",
            "Epoch [   81/  100] | d_loss: 1.2087 | g_loss: 1.0618\n",
            "Epoch [   81/  100] | d_loss: 1.2772 | g_loss: 1.2357\n",
            "Epoch [   82/  100] | d_loss: 1.2920 | g_loss: 0.8625\n",
            "Epoch [   82/  100] | d_loss: 1.2205 | g_loss: 1.0187\n",
            "Epoch [   82/  100] | d_loss: 1.3755 | g_loss: 1.0526\n",
            "Epoch [   83/  100] | d_loss: 1.2403 | g_loss: 1.1032\n",
            "Epoch [   83/  100] | d_loss: 1.2369 | g_loss: 0.9357\n",
            "Epoch [   83/  100] | d_loss: 1.2854 | g_loss: 0.9699\n",
            "Epoch [   84/  100] | d_loss: 1.3513 | g_loss: 1.3192\n",
            "Epoch [   84/  100] | d_loss: 1.1412 | g_loss: 1.0437\n",
            "Epoch [   84/  100] | d_loss: 1.3631 | g_loss: 0.9049\n",
            "Epoch [   85/  100] | d_loss: 1.2344 | g_loss: 1.3532\n",
            "Epoch [   85/  100] | d_loss: 1.1593 | g_loss: 1.3618\n",
            "Epoch [   85/  100] | d_loss: 1.4354 | g_loss: 1.0481\n",
            "Epoch [   86/  100] | d_loss: 1.3441 | g_loss: 0.9278\n",
            "Epoch [   86/  100] | d_loss: 1.2359 | g_loss: 1.1224\n",
            "Epoch [   86/  100] | d_loss: 1.3065 | g_loss: 0.8476\n",
            "Epoch [   87/  100] | d_loss: 1.2853 | g_loss: 1.0951\n",
            "Epoch [   87/  100] | d_loss: 1.3089 | g_loss: 0.9532\n",
            "Epoch [   87/  100] | d_loss: 1.2958 | g_loss: 1.0773\n",
            "Epoch [   88/  100] | d_loss: 1.2889 | g_loss: 0.9741\n",
            "Epoch [   88/  100] | d_loss: 1.2086 | g_loss: 1.1043\n",
            "Epoch [   88/  100] | d_loss: 1.2218 | g_loss: 1.2442\n",
            "Epoch [   89/  100] | d_loss: 1.2978 | g_loss: 0.8967\n",
            "Epoch [   89/  100] | d_loss: 1.3322 | g_loss: 1.0873\n",
            "Epoch [   89/  100] | d_loss: 1.3525 | g_loss: 1.1595\n",
            "Epoch [   90/  100] | d_loss: 1.3654 | g_loss: 1.3753\n",
            "Epoch [   90/  100] | d_loss: 1.3860 | g_loss: 1.1769\n",
            "Epoch [   90/  100] | d_loss: 1.2798 | g_loss: 1.1113\n",
            "Epoch [   91/  100] | d_loss: 1.2325 | g_loss: 1.4243\n",
            "Epoch [   91/  100] | d_loss: 1.2612 | g_loss: 0.9955\n",
            "Epoch [   91/  100] | d_loss: 1.3831 | g_loss: 1.0972\n",
            "Epoch [   92/  100] | d_loss: 1.2950 | g_loss: 0.9687\n",
            "Epoch [   92/  100] | d_loss: 1.1960 | g_loss: 0.9626\n",
            "Epoch [   92/  100] | d_loss: 1.2820 | g_loss: 1.4049\n",
            "Epoch [   93/  100] | d_loss: 1.3361 | g_loss: 0.9633\n",
            "Epoch [   93/  100] | d_loss: 1.3622 | g_loss: 0.9282\n",
            "Epoch [   93/  100] | d_loss: 1.4243 | g_loss: 1.0687\n",
            "Epoch [   94/  100] | d_loss: 1.2935 | g_loss: 1.0341\n",
            "Epoch [   94/  100] | d_loss: 1.1607 | g_loss: 1.3840\n",
            "Epoch [   94/  100] | d_loss: 1.3062 | g_loss: 0.9081\n",
            "Epoch [   95/  100] | d_loss: 1.2942 | g_loss: 0.9543\n",
            "Epoch [   95/  100] | d_loss: 1.1888 | g_loss: 0.9583\n",
            "Epoch [   95/  100] | d_loss: 1.3482 | g_loss: 1.0577\n",
            "Epoch [   96/  100] | d_loss: 1.3505 | g_loss: 0.7918\n",
            "Epoch [   96/  100] | d_loss: 1.3128 | g_loss: 1.0004\n",
            "Epoch [   96/  100] | d_loss: 1.3307 | g_loss: 1.0302\n",
            "Epoch [   97/  100] | d_loss: 1.2090 | g_loss: 1.4004\n",
            "Epoch [   97/  100] | d_loss: 1.3467 | g_loss: 0.9499\n",
            "Epoch [   97/  100] | d_loss: 1.4037 | g_loss: 1.0529\n",
            "Epoch [   98/  100] | d_loss: 1.3925 | g_loss: 1.1781\n",
            "Epoch [   98/  100] | d_loss: 1.2957 | g_loss: 0.9203\n",
            "Epoch [   98/  100] | d_loss: 1.2758 | g_loss: 1.1105\n",
            "Epoch [   99/  100] | d_loss: 1.1764 | g_loss: 0.9913\n",
            "Epoch [   99/  100] | d_loss: 1.2897 | g_loss: 0.9697\n",
            "Epoch [   99/  100] | d_loss: 1.3851 | g_loss: 1.0603\n",
            "Epoch [  100/  100] | d_loss: 1.2867 | g_loss: 0.8937\n",
            "Epoch [  100/  100] | d_loss: 1.1564 | g_loss: 1.0133\n",
            "Epoch [  100/  100] | d_loss: 1.3332 | g_loss: 0.9537\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}